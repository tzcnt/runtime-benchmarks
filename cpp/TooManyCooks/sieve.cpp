// Modified from:
// https://github.com/lums658/sieve_exec_comparison/blob/a682327b58d969a16b69edfafc5436735eeaded6/sieve/sieve_cc_fun.cpp

// Original license:
/**
 * @file sieve_cc_fun.cpp
 *
 * @section LICENSE
 *
 * The MIT License
 *
 * @copyright Copyright (c) 2022 TileDB, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 * @section DESCRIPTION
 *
 * Demo program for concurrencpp system: sieve of Eratosthenes,
 * free function explicit coroutine version.
 *
 * The functions that are composed for the sieve are defined in
 * sieve_fun.hpp.
 *
 * The functions in this implementation are chained together by essentially
 * calling each other in a chain, but with co_await and co_return inserted
 * in that composition.  Read-only state variables are also passed in to
 * the functions that need them.  See the definition of "task" below.
 */
#define TMC_IMPL

#include "sieve.hpp"
#include "sieve_fun.hpp"
#include "tmc/all_headers.hpp"
#include "tmc/ex_cpu.hpp"
#include "tmc/sync.hpp"
#include <atomic>

static size_t thread_count = std::thread::hardware_concurrency() / 2;

template <typename bool_t>
tmc::task<void> sieve_one(
  size_t p, size_t n, size_t sqrt_n, size_t block_size,
  std::vector<size_t>& base_primes,
  std::vector<std::shared_ptr<std::vector<size_t>>>& prime_list
) {
  auto partinfo = gen_range<bool_t>(p, block_size, sqrt_n, n);
  co_await range_sieve<bool_t>(partinfo, base_primes);
  auto [pi, primes] = sieve_to_primes_part<bool_t>(partinfo);
  prime_list[pi] = primes;

  co_return;
}

/**
 * Main sieve function
 *
 * @brief Generate primes from 2 to n using sieve of Eratosthenes.
 * @tparam bool_t the type to use for the "bitmap"
 * @param n upper bound of sieve
 * @param block_size how many primes to search for given a base set of primes
 */
template <class bool_t> auto sieve_block(size_t n, size_t block_size) {
  size_t sqrt_n = static_cast<size_t>(std::ceil(std::sqrt(n)));

  /* Generate base set of sqrt(n) primes to be used for subsequent sieving */
  auto first_sieve = sieve_seq<bool_t>(sqrt_n);
  std::vector<size_t> base_primes = sieve_to_primes(first_sieve);

  /* Store vector of list of primes (each list generated by separate task chain)
   */
  std::vector<std::shared_ptr<std::vector<size_t>>> prime_list(
    n / block_size + 2
  );
  prime_list[0] = std::make_shared<std::vector<size_t>>(base_primes);

  /**
   * Task containing chain of coroutine calls for generating primes
   * @param cc::executor_tag tells the runtime system to launch in parallel
   */
  input_body gen{};
  //  auto task = [&]() -> tmc::task<void> {
  //    co_return output_body(
  //      sieve_to_primes_part<bool_t>(range_sieve<bool_t>(
  //        gen_range<bool_t>(gen(), block_size, sqrt_n, n), base_primes
  //      )),
  //      prime_list
  //    );
  //  };

  /**
   * Launch tasks, each of which computes a  block of primes
   */
  std::vector<tmc::task<void>> tasks;
  for (size_t i = 0; i < n / block_size + 1; ++i) {
    tasks.push_back(
      sieve_one<bool_t>(gen(), n, sqrt_n, block_size, base_primes, prime_list)
    );
  }
  tmc::post_bulk_waitable(tmc::cpu_executor(), tasks.begin(), tasks.size())
    .wait();

  /**
   * Wait for tasks to complete
   */

  return prime_list;
}

int main(int argc, char* argv[]) {
  size_t number = 100'000'000;
  size_t block_size = 100;

  if (argc >= 2) {
    number = std::stol(argv[1]);
  }
  if (argc >= 3) {
    block_size = std::stol(argv[2]);
  }

  std::printf("threads: %zu\n", thread_count);
  tmc::cpu_executor().set_thread_count(thread_count).init();

  {
    // warmup
    auto result = sieve_block<char>(number, block_size * 1024);
  }

  auto startTime = std::chrono::high_resolution_clock::now();

  auto result = sieve_block<char>(number, block_size * 1024);
  size_t num = 0;
  for (auto& j : result) {
    if (j) {
      num += j->size();
    }
  }
  std::cout << num << ": " << std::endl;

  auto endTime = std::chrono::high_resolution_clock::now();
  auto totalTimeUs =
    std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime);
  std::printf("runs:\n");
  std::printf("    duration: %zu us\n", totalTimeUs.count());
}
